


## 注意事項:
上傳下載使用 UI URL ip會動態帶入 外部訪問使用 docker 需要使用暴露與端口
Datanode 無暴露端口需要使用指令查詢docker容器ip
docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' e66558c3d824

# core-site.xml 配置
CORE-SITE.XML_fs.default.name=hdfs://namenode  # HDFS 默認命名空間地址
CORE-SITE.XML_fs.defaultFS=hdfs://namenode  # HDFS 默認文件系統
CORE-SITE.XML_hadoop.http.staticuser.user=root  # 設置 HTTP 用戶為 root
CORE-SITE.XML_hadoop.tmp.dir=/tmp/hadoop-root  # Hadoop 臨時目錄

# hdfs-site.xml 配置
HDFS-SITE.XML_dfs.namenode.rpc-address=namenode:8020  # NameNode RPC 服務器地址
HDFS-SITE.XML_dfs.replication=2  # HDFS 副本因子（設置為 2 表示每個數據塊有兩個副本）

# mapred-site.xml 配置
MAPRED-SITE.XML_mapreduce.framework.name=yarn  # MapReduce 計算框架使用 YARN
MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=${HADOOP_HOME}  # 設置 AM（Application Master）環境變量
MAPRED-SITE.XML_mapreduce.map.env=HADOOP_MAPRED_HOME=${HADOOP_HOME}  # 設置 Map 任務環境變量
MAPRED-SITE.XML_mapreduce.reduce.env=HADOOP_MAPRED_HOME=${HADOOP_HOME}  # 設置 Reduce 任務環境變量
MAPRED-SITE.XML_mapreduce.jobhistory.address=0.0.0.0:10020  # JobHistory Server 地址
MAPRED-SITE.XML_mapreduce.jobhistory.webapp.address=0.0.0.0:19888  # JobHistory Server Web UI 端口

# yarn-site.xml 配置
YARN-SITE.XML_yarn.resourcemanager.hostname=resourcemanager  # YARN ResourceManager 主機名
YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled=true  # 啟用物理內存檢查 *java8 和 linux 虛擬內存兼容性不好java會沒法用 開啟物理內存
YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec=600  # 刪除容器日誌的延遲時間（單位：秒）
YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled=true  # 啟用虛擬內存檢查
YARN-SITE.XML_yarn.nodemanager.aux-services=mapreduce_shuffle  # 設置輔助服務為 mapreduce_shuffle
YARN-SITE.XML_yarn.nodemanager.resource.cpu-vcores=4  # 設置每個 NodeManager 提供的 CPU 核數
YARN-SITE.XML_yarn.log-aggregation-enable=true  # 啟用 YARN 日誌聚合
YARN-SITE.XML_yarn.log.server.url=http://namenode:19888/jobhistory/logs  # 指定 YARN 日誌伺服器 URL
YARN-SITE.XML_yarn.log-aggregation.retain-seconds=604800  # 指定日誌在 HDFS 中保留的時間（秒）
YARN-SITE.XML_yarn.application.classpath=opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*
# 以上設置 YARN 任務運行時的 CLASSPATH，確保 Hadoop 組件可用

# capacity-scheduler.xml 配置
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications=10000  # 最大支持的應用數量
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent=0.1  # AM 資源最大佔比
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator  # 資源計算方式
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues=default  # 根隊列設置
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity=100  # 默認隊列的容量（百分比）
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor=1  # 用戶最大可用資源係數
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity=100  # 隊列最大容量
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state=RUNNING  # 隊列當前狀態
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_submit_applications=*  # 允許所有用戶提交應用
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_administer_queue=*  # 允許所有用戶管理隊列
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay=40  # 本地性延遲（任務優先在本地運行）
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings=  # 隊列映射（留空表示無映射）
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable=false  # 是否啟用隊列映射重寫



-------- 其他配置 -------

---HDFS-SITE.XML:
處理不同 DataNode 的併發心跳及客戶端病發的元數據操作 dfs.namenode.handler.count=20Xlog Cluster Size:
dfs.namenode.handler.count=21

擴容用 第一次要重啟集群  之後使用 myhadoop.sh stop , myhadoop.sh start
白名單(需要在該路徑下添加whitelist)
dfs.hosts=/opt/module/hadoop-3.1.3/etc/hadoop/whitelist
刷新NameNode
hdfs dfsadmin -refreshNodes
数据均衡
sbin/start-balancer.sh -threshold 10

縮容用 第一次要重啟集群  之後使用 myhadoop.sh stop , myhadoop.sh start
黑名單白名單(需要在該路徑下添加blacklist)
dfs.hosts.exclude=/opt/module/hadoop-3.1.3/etc/hadoop/blacklist
刷新NameNode
hdfs dfsadmin -refreshNodes
数据均衡
sbin/start-balancer.sh -threshold 10




---CORE-SITE.XML:

配置垃圾回收 1 分鐘 **web上刪除不會走回收站 代碼要通過new_Trash(conf) 移動到回收站
hadoop fs -rm /a.txt 會走回收站
fs.trash.interval=1




---DFS-SITE.XML:

DataNode多目錄配置 ( 同個節點多個 DataNode 添加新的硬碟 ) *每隔節點硬體不同要分別對應
dfs.datanode.data.dir=file://{hadoop.tmp.dir}/dfs/data1,file://{hadoop.tmp.dir}/dfs/data2
添加硬碟並設置結舔後需數據均衡 同一節點附載均衡:
生成均衡計畫:
hdfs diskbalancer -plan hadoop103
執行均衡計畫:
hdfs diskbalancer -execute hadoop103.plan.json
查看當前均衡計畫執行情況:
hdfs diskbalancer -query hadoop103
取消均衡計畫:
hdfs diskbalancer -cancel hadoop103.plan.json










